# -*- coding: utf-8 -*-
"""RL_DVFS_TRAIN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1xcl_D-ySfV3yf3NzEiAL3jl4EbA2LOe4
"""

import numpy as np
import torch as T
import torch.nn as nn
import torch.optim as optim
import pandas as pd
from pathlib import Path
import datetime
import copy
import os

# CREATING A CLASS FOR DEFINING THE FUNCTION APPROXIMATOR FOR DQN
class DQN_PRED(nn.Module):
    def __init__(self, lr):
        super().__init__()
        self.lin = nn.Sequential(
                                nn.Linear(5, 256, bias=True),
                                nn.ReLU(inplace=True),
                                nn.Linear(256, 256, bias=True),
                                nn.ReLU(inplace=True),
                                nn.Linear(256, 10, bias=True)
                                )
        self.tgt = copy.deepcopy(self.lin)
        for p in self.tgt.parameters():
          p.requires_grad = False

    def forward(self, x, model):
        #inp = T.tensor(x)
        inp = T.flatten(x, start_dim=1)
        if model == 'online':
          return self.lin(inp)
        else:
          return self.tgt(inp)

class Agent(object):
    def __init__(self, gamma = 0.999, epsilon = 1, lr = 0.1, batch_size = 128):
        self.gamma = gamma                                                                  # DISCOUNT FACTOR
        #self.epsilon = epsilon                                                              # EPS FOR EPSILON-GREEDY
        self.lr = lr                                                                        # LEARNING RATE FOR THE GRADIENT DESCENT GIVEN THROUGH OPTIMISER
        self.batch_size = batch_size      
        self.mem_cntr = 0       
        self.log = 1e3    
        self.tgt_update = 1000
        self.device = 'cuda' if T.cuda.is_available() else 'cpu'                           

        self.Q_eval = DQN_PRED(self.lr).to(self.device)                                                          # INSTANCE FOR NEURAL NETWORK CLASS

        self.optimizer = optim.Adam(self.Q_eval.parameters(),lr=lr)
        self.scheduler = optim.lr_scheduler.ExponentialLR(self.optimizer, 0.999)

        self.loss_fn = nn.MSELoss()                                                            # MSE LOSS USED, (Q_TARGET - Q_PRED) ** 2
  
        self.state_memory, self.action_memory, self.reward_memory = self.load_data()                                 # (10000,4) STATE VECTOR CONTAINS 4 VALUES
            
        #self.terminal_memory = np.zeros(self.mem_size, dtype=np.float32)                    # STORES WHETHER EPISODE IS COMPLETE OR NOT

        self.save_dir = Path('/content/drive/MyDrive/RL_DVFS')
    
    def load_data(self):
        s, a, r = [], [], []
        for i, j in enumerate(os.listdir(r'/content/drive/MyDrive/RL_DVFS/DATA')):
          x = self.str_to_numpy(pd.read_csv(f'//content//drive//MyDrive//RL_DVFS//DATA//{j}',delimiter=',', dtype='object'))
          s.append(x[0])
          a.append(x[1])
          r.append(x[2])
        s, a, r = np.array(s), np.array(a), np.array(r)
        s, a, r = s.reshape((i+1)*1000, 5), a.reshape((i+1)*1000,1), r.reshape((i+1)*1000,1)
        return s, a, r

    def str_to_numpy(self, data):
        state = []
        action = []
        reward = []
        for S, A, R in zip(data.iloc[0,1:], data.iloc[1,1:], data.iloc[2,1:]):
          state.append(np.fromstring(S[2:-2], dtype=np.float, sep=' '))
          action.append(np.fromstring(A[:], dtype=np.float, sep=' '))
          reward.append(np.fromstring(R[:], dtype=np.float, sep=' '))
        return np.array(state), np.array(action), np.array(reward)
    
    def save(self):
        # We use Path from pathlib so directly we can put / for the path.
        save_path = self.save_dir / f"Model_DVFS_{int(self.mem_cntr // self.log)}_{datetime.datetime.now()}.pth.tar"

        T.save(self.Q_eval.state_dict(), save_path)
        
        print(f"MODEL saved to {save_path}")

    def learn(self):
        #if self.mem_cntr % self.log == 0:
        #    self.save()
        if self.mem_cntr and self.mem_cntr % self.tgt_update == 0:
          self.Q_eval.tgt.load_state_dict(self.Q_eval.lin.state_dict())

        batch = np.random.choice(999, self.batch_size, replace=False)                              # SELECTING BATCHSIZE NO. OF SAMPLES FROM MAXMEM RANDOMLY

        state_batch = T.Tensor(self.state_memory[batch]).to(self.device)
        action_batch = T.tensor(self.action_memory[batch], dtype = T.long).squeeze(1).to(self.device)           
        reward_batch = T.Tensor(self.reward_memory[batch]).squeeze(1).to(self.device)                              # COLLECTING REQUIRED PARAMETERS FROM TRANSITION MEMORY FOR A GIVEN BATCH
        #terminal_batch = T.Tensor(self.terminal_memory[batch])
        new_state_batch = T.Tensor(self.state_memory[batch+1]).to(self.device)
        batch_index = np.arange(self.batch_size, dtype=np.int32)                        # BATCHINDEX VALUES FOR UPDATING THE Q_TARGET

        #print(self.Q_eval.forward(state_batch, 'online'), [batch_index, action_batch])
        q_s_a = self.Q_eval.forward(state_batch, 'online')[batch_index, action_batch]             # Q_PRED FOR THE UPDATE

        best_action = T.argmax(self.Q_eval.forward(new_state_batch, 'online'), axis = 1)
        q_S_A = self.Q_eval.forward(new_state_batch, 'tgt')[batch_index, best_action]                                    # MAXIMUM VALUE FROM NEXT STATE
        
        q_target = reward_batch + self.gamma * q_S_A
        #print(f'Q_s_a {q_s_a} \n Action {action_batch} \n rew {reward_batch} \n Max_ns{q_S_A} \n Q_TGT {q_target}')

        loss = self.loss_fn(q_s_a, q_target)                                        # Q_TARGET - Q_PRED IS THE TD-ERROR AND MSE LOSS FINDS THE SQUARE OF IT
        self.optimizer.zero_grad()                                               # FOR NULLIFYING GRADIENTS OF PREVIOUS BATCHES
        loss.backward()                                                                 # BACKPROPAGATING THE LOSS TO FING GRADIENTS

        self.optimizer.step()                                                    # UPDATING THE PARAMETERS I.E WEIGHTS USING GRADIENT DESCENT FOR THE Q_VALUE FUNCTION
        self.scheduler.step()
        #print(f'LOSS : {loss.item()}\nnew_pred : {self.Q_eval.forward(state_batch[0])}\n{T.argmax(self.Q_eval.forward(state_batch[0]))}')

        self.mem_cntr += 1

        return loss.item()

agent = Agent(lr=0.025, batch_size=256)

import math
a = [math.ceil(x*80) for x in agent.state_memory[:,0]]
print('TEMP', set(a))

a = [math.ceil(x*100) for x in agent.state_memory[:,1]]
print('UTI', set(a))

a = [math.ceil(x*100) for x in agent.state_memory[:,2]]
print('AVG-UTI', set(a))

set(agent.action_memory[:,0])

#agent.Q_eval.load_state_dict(T.load(r'/content/drive/MyDrive/RL_DVFS/Model_DVFS_0_2021-06-28 14:18:14.522684.chkpt'))
loss = []
for i in range(50000):
  if i % 500 == 0:
    l = agent.learn()
    loss.append(l)
    #print(f'E:{i:>4}\tLOSS : {l:>7}')

# Commented out IPython magic to ensure Python compatibility.
import matplotlib.pyplot as plt
# %matplotlib inline

plt.plot(loss)

i = 0.15
while i <= 1:
  inp = T.Tensor([[0.94, i, i, 1- i, 4]]).unsqueeze(0)
  pred = agent.Q_eval.forward(inp, 'online')
  print(*pred.data[0].numpy())
  print(f'\n{int((i*100)//10)} : {inp.data[0][0][0].item()*80:.2f} {inp.data[0][0][1].item()*100:.2f} {inp.data[0][0][2].item()*100:.2f} {inp.data[0][0][3].item()*100:.2f} {inp.data[0][0][4].item():.2f} ACTION : {T.argmax(pred).item()} \n')
  i += 0.05

inp = T.Tensor([[0.163, 0.12, 0.12, 0.8, 0.04]]).unsqueeze(0)
pred = agent.Q_eval.forward(inp, 'online')
print(inp.data, '\n', pred.data, '\n', T.argmax(pred).item())

inp = T.Tensor([[0.863, 0.9, 0.9, 0.1, 5.00]]).unsqueeze(0)
pred = agent.Q_eval.forward(inp, 'online')
print(inp.data, '\n', pred.data, '\n', T.argmax(pred).item())

save_path = Path('/content/drive/MyDrive/RL_DVFS') / f"Model_DVFS_{int(agent.mem_cntr // agent.log)}_{datetime.datetime.now()}.pth.tar"
T.save(agent.Q_eval.state_dict(), save_path, _use_new_zipfile_serialization=False)